{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1\n",
    "ANSWER==\n",
    "# Given probabilities\n",
    "P_H = 0.70  # Probability that an employee uses the health insurance plan\n",
    "P_S_given_H = 0.40  # Probability that an employee is a smoker given that they use the health insurance plan\n",
    "\n",
    "# Calculate the probability that an employee is a smoker given that he/she uses the health insurance plan\n",
    "P_S_and_H = P_H * P_S_given_H  # Probability of an employee being both a smoker and using the health insurance plan\n",
    "P_S = P_S_and_H / P_H  # Total probability of an employee being a smoker\n",
    "print(\"Probability that an employee is a smoker given that he/she uses the health insurance plan:\", P_S)\n",
    "\n",
    "\n",
    "Probability that an employee is a smoker given that he/she uses the health insurance plan: 0.40\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2=\n",
    "ANSWER==# Difference Between Bernoulli Naive Bayes and Multinomial Naive Bayes\n",
    "\n",
    "## Bernoulli Naive Bayes\n",
    "- **Feature Type**: Designed for binary/boolean features.\n",
    "- **Application**: Best suited for scenarios where features represent binary occurrences such as \"word present\" or \"word absent\".\n",
    "- **Example Use Case**: Text classification with binary term occurrence (whether a word appears in a document or not).\n",
    "- **Probability Calculation**: Considers the presence or absence of a feature. It uses Bernoulli distribution for calculating probabilities.\n",
    "- **Handling Missing Features**: Considers the absence of a feature as significant, thus it can handle sparse data well.\n",
    "\n",
    "## Multinomial Naive Bayes\n",
    "- **Feature Type**: Designed for discrete features representing counts.\n",
    "- **Application**: Best suited for scenarios where features represent count data, such as the frequency of words in a document.\n",
    "- **Example Use Case**: Text classification with term frequency counts (how many times a word appears in a document).\n",
    "- **Probability Calculation**: Considers the frequency of features. It uses multinomial distribution for calculating probabilities.\n",
    "- **Handling Missing Features**: Does not consider the absence of a feature directly, focuses on the count of features present.\n",
    "\n",
    "## Key Differences\n",
    "- **Feature Representation**:\n",
    "  - Bernoulli Naive Bayes: Binary features.\n",
    "  - Multinomial Naive Bayes: Count-based features.\n",
    "  \n",
    "- **Distribution Assumption**:\n",
    "  - Bernoulli Naive Bayes: Bernoulli distribution.\n",
    "  - Multinomial Naive Bayes: Multinomial distribution.\n",
    "  \n",
    "- **Data Suitability**:\n",
    "  - Bernoulli Naive Bayes: Suitable for binary data and sparse datasets.\n",
    "  - Multinomial Naive Bayes: Suitable for count data and document classification.\n",
    "\n",
    "## Conclusion\n",
    "Choosing between Bernoulli Naive Bayes and Multinomial Naive Bayes depends on the nature of your features:\n",
    "- Use **Bernoulli Naive Bayes** for binary features.\n",
    "- Use **Multinomial Naive Bayes** for count features.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3=\n",
    "ANSWER=# Handling Missing Values in Bernoulli Naive Bayes\n",
    "\n",
    "Bernoulli Naive Bayes handles missing values implicitly by considering the absence of a feature as informative. In this model, if a feature is missing, it is treated as if it were not observed, implying that the corresponding attribute is absent. Consequently, the absence of a feature contributes to the likelihood calculation, affecting the probability estimation. This implicit handling of missing values aligns with the assumption that the presence or absence of a feature is crucial for classification in Bernoulli Naive Bayes. Therefore, missing values are effectively incorporated into the model through the absence of features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4=\n",
    "ANSWER==\n",
    "# Multi-Class Classification with Gaussian Naive Bayes\n",
    "\n",
    "Yes, Gaussian Naive Bayes can be used for multi-class classification tasks. While Gaussian Naive Bayes is often used for binary classification, it can also be extended to handle multiple classes. In the case of multi-class classification, the model calculates the probability of each class given the input features using Gaussian probability density function (PDF) assuming that the features are continuous and follow a Gaussian distribution. The class with the highest probability is then predicted as the output. Therefore, Gaussian Naive Bayes is a versatile algorithm capable of handling both binary and multi-class classification problems.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q1=\n",
    "# ANSWER==\n",
    "# Implementation of Naive Bayes Classifiers for Spam Classification\n",
    "\n",
    "## Data Preparation\n",
    "Download the \"Spambase Data Set\" from the UCI Machine Learning Repository and preprocess the data as needed.\n",
    "\n",
    "## Implementation\n",
    "Implement Bernoulli Naive Bayes, Multinomial Naive Bayes, and Gaussian Naive Bayes classifiers using scikit-learn.\n",
    "\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Load the Spambase dataset\n",
    "data = fetch_openml(name='spambase')\n",
    "\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Instantiate classifiers\n",
    "bernoulli_nb = BernoulliNB()\n",
    "multinomial_nb = MultinomialNB()\n",
    "gaussian_nb = GaussianNB()\n",
    "\n",
    "# Perform 10-fold cross-validation\n",
    "accuracy_bernoulli = cross_val_score(bernoulli_nb, X, y, cv=10, scoring='accuracy')\n",
    "accuracy_multinomial = cross_val_score(multinomial_nb, X, y, cv=10, scoring='accuracy')\n",
    "accuracy_gaussian = cross_val_score(gaussian_nb, X, y, cv=10, scoring='accuracy')\n",
    "\n",
    "# Report performance metrics\n",
    "print(\"Bernoulli Naive Bayes:\")\n",
    "print(\"Accuracy:\", accuracy_bernoulli.mean())\n",
    "print(\"Precision:\", precision_score(y_true, y_pred))\n",
    "print(\"Recall:\", recall_score(y_true, y_pred))\n",
    "print(\"F1 Score:\", f1_score(y_true, y_pred))\n",
    "\n",
    "print(\"Multinomial Naive Bayes:\")\n",
    "print(\"Accuracy:\", accuracy_multinomial.mean())\n",
    "print(\"Precision:\", precision_score(y_true, y_pred))\n",
    "print(\"Recall:\", recall_score(y_true, y_pred))\n",
    "print(\"F1 Score:\", f1_score(y_true, y_pred))\n",
    "\n",
    "print(\"Gaussian Naive Bayes:\")\n",
    "print(\"Accuracy:\", accuracy_gaussian.mean())\n",
    "print(\"Precision:\", precision_score(y_true, y_pred))\n",
    "print(\"Recall:\", recall_score(y_true, y_pred))\n",
    "print(\"F1 Score:\", f1_score(y_true, y_pred))\n",
    "\n",
    "'''Discussion\n",
    "Discuss the results obtained from each classifier, including their performance metrics. Analyze which variant of Naive Bayes performed the best and why. Discuss any limitations observed.\n",
    "'''\n",
    "\n",
    "'''Conclusion\n",
    "Summarize the findings from the evaluation of Naive Bayes classifiers for spam classification. Provide suggestions for future work, such as exploring other classification algorithms or optimizing hyperparameters for improved performance.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

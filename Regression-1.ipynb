{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1----\n",
    "Answer---Simple linear regression models the relationship between one predictor variable and one response variable using the equation \n",
    "𝑌=𝛽0+𝛽1𝑋1+𝜖 Example: Predicting exam scores based on hours studied.\n",
    "\n",
    "Multiple linear regression models the relationship between multiple predictor variables and one response variable using the equation \n",
    "𝑌=𝛽0+𝛽1𝑋1+𝛽2𝑋2+⋯+𝛽𝑝𝑋𝑝+𝜖\n",
    "Example: Predicting exam scores based on hours studied and attendance rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2----\n",
    "Answer---\n",
    "Assumptions of Linear Regression\n",
    "Linearity: The relationship between the predictors and the response variable is linear.\n",
    "Independence: The residuals (errors) are independent of each other.\n",
    "Homoscedasticity: The residuals have constant variance at every level of the predictor.\n",
    "Normality: The residuals of the model are normally distributed.\n",
    "No multicollinearity: In multiple regression, the predictor variables are not highly correlated with each other.\n",
    "How to Check Assumptions\n",
    "Linearity:\n",
    "\n",
    "Visual Inspection: Plot the predicted values versus the observed values or plot each predictor against the response variable.\n",
    "Residual Plots: Plot residuals against predicted values; non-random patterns suggest non-linearity.\n",
    "Independence:\n",
    "\n",
    "Durbin-Watson Test: This test detects the presence of autocorrelation in the residuals.\n",
    "Residual Plots: Check for patterns in residuals plotted against time or other indices.\n",
    "Homoscedasticity:\n",
    "\n",
    "Residual Plots: Plot residuals against predicted values; a funnel shape suggests heteroscedasticity.\n",
    "Breusch-Pagan Test: This statistical test checks for heteroscedasticity.\n",
    "Normality:\n",
    "\n",
    "Q-Q Plot: Plot the quantiles of residuals against the quantiles of a normal distribution; points should lie approximately along a straight line.\n",
    "Shapiro-Wilk Test: This test assesses the normality of residuals.\n",
    "No Multicollinearity:\n",
    "\n",
    "Variance Inflation Factor (VIF): Calculate VIF for each predictor; values greater than 10 suggest high multicollinearity.\n",
    "Correlation Matrix: Check the correlation coefficients between predictors; high correlations indicate multicollinearity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3----\n",
    "Answer----\n",
    "Interpreting the Slope and Intercept in a Linear Regression Model\n",
    "Intercept (β0): The intercept is the expected value of the response variable (𝑌) when all predictor variables (𝑋) are zero. It represents the baseline level of Y without any influence from the predictors.\n",
    "\n",
    "Slope (𝛽1): The slope indicates the expected change in the response variable (𝑌) for a one-unit increase in the predictor variable (X), holding all other variables constant. It shows the strength and direction of the relationship between X and 𝑌.\n",
    "\n",
    "Real-World Scenario Example\n",
    "Scenario:\n",
    "Suppose we want to model the relationship between the number of hours studied (predictor variable,X) and exam scores (response variable,Y) for students.\n",
    "Linear Regression Equation:𝑌=𝛽0+𝛽1𝑋+𝜖\n",
    "\n",
    "Let's say our fitted regression equation from the data is:\n",
    "Exam Score = 50+5×Hours Studied\n",
    "Intercept (𝛽0 = 50): The intercept of 50 means that if a student studies 0 hours, their expected exam score is 50. This represents the baseline score without any study.\n",
    "\n",
    "Slope (𝛽1 = 5): The slope of 5 means that for each additional hour a student studies, their exam score is expected to increase by 5 points. This shows a positive relationship between hours studied and exam scores.\n",
    "\n",
    "Interpretation:\n",
    "If a student does not study at all (0 hours), they are expected to score 50 points on the exam.\n",
    "For each additional hour a student studies, their exam score increases by 5 points. So, if a student studies for 3 hours, their expected exam score would be:\n",
    "Exam Score=50+5×3=65\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4----\n",
    "Answer---\n",
    "\n",
    "Gradient descent is an optimization algorithm used to minimize a function by iteratively moving towards the steepest descent, i.e., the direction of the negative gradient. In machine learning, it optimizes the model's parameters (weights) by reducing the error between predicted and actual values. Starting with random parameters, gradient descent updates them iteratively using the gradient of the loss function, gradually converging to the optimal values that minimize the error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5--\n",
    "Answer--\n",
    "Multiple Linear Regression Model\n",
    "Multiple linear regression models the relationship between one response variable and multiple predictor variables. The model is represented by:\n",
    "𝑌=𝛽0+𝛽1𝑋1+𝛽2X2+⋯+𝛽𝑝𝑋𝑝+𝜖\n",
    "\n",
    "Differences from Simple Linear Regression\n",
    "Number of Predictors: Multiple linear regression uses multiple predictors, while simple linear regression uses one.\n",
    "Model Complexity: Multiple linear regression considers combined effects of several predictors, unlike simple linear regression, which only considers the effect of one predictor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q6--\n",
    "Answer--\n",
    "Multicollinearity in Multiple Linear Regression\n",
    "Multicollinearity occurs when predictor variables in a multiple regression model are highly correlated, leading to unreliable coefficient estimates. It can inflate standard errors, making it difficult to assess the effect of each predictor.\n",
    "\n",
    "Detection\n",
    "Variance Inflation Factor (VIF): VIF values above 10 indicate high multicollinearity.\n",
    "Correlation Matrix: High correlation coefficients between predictors suggest multicollinearity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q7--\n",
    "Answer--\n",
    "Polynomial Regression Model\n",
    "Polynomial regression extends linear regression by fitting a curve (polynomial function) to the data, allowing for non-linear relationships between predictors and the response variable. It's represented as:\n",
    "𝑌=𝛽0+𝛽1𝑋1+𝛽1𝑋^2+--+𝛽𝑛𝑋^2+𝜖\n",
    "\n",
    "Differences from Linear Regression\n",
    "Polynomial regression can capture non-linear relationships.\n",
    "It involves higher-order terms (𝑋^2,𝑋^3), unlike linear regression which uses only first-order terms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q8--\n",
    "Answer--Advantages and Disadvantages of Polynomial Regression\n",
    "Advantages:\n",
    "\n",
    "Captures non-linear relationships between variables.\n",
    "More flexible than linear regression.\n",
    "Disadvantages:\n",
    "\n",
    "Can overfit data, especially with higher-degree polynomials.\n",
    "Interpretation becomes more complex with higher-degree polynomials.\n",
    "Preferred Use:\n",
    "Polynomial regression is preferred when the relationship between variables is non-linear and cannot be adequately captured by linear regression, such as in curved or oscillating patterns in the data."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

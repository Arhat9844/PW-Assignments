{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1--\n",
    "Answer-\n",
    "Steps:\n",
    "Automated Feature Selection\n",
    "Numerical Pipeline\n",
    "Impute missing values using the mean\n",
    "Scale the numerical columns using standardization\n",
    "Categorical Pipeline\n",
    "Impute missing values using the most frequent value\n",
    "One-hot encode the categorical columns\n",
    "Combine Pipelines using ColumnTransformer\n",
    "Build and Train the Model using Random Forest Classifier\n",
    "Evaluate the Model\n",
    "1. Automated Feature Selection\n",
    "We'll use a feature selection method such as Recursive Feature Elimination (RFE) with a Random Forest Classifier to identify important features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "# Load your dataset\n",
    "data = pd.read_csv('your_dataset.csv')\n",
    "\n",
    "# Separate features and target\n",
    "X = data.drop('target_column', axis=1)\n",
    "y = data['target_column']\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the model\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Use RFE for feature selection\n",
    "selector = RFE(model, n_features_to_select=10, step=1)\n",
    "selector = selector.fit(X_train, y_train)\n",
    "\n",
    "# Get the selected features\n",
    "selected_features = X_train.columns[selector.support_]\n",
    "X_train = X_train[selected_features]\n",
    "X_test = X_test[selected_features]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Numerical Pipeline\n",
    "Step 1: Impute Missing Values using Mean\n",
    "\n",
    "Step 2: Scale the Numerical Columns using Standardization\n",
    "\n",
    "3. Categorical Pipeline\n",
    "Step 1: Impute Missing Values using Most Frequent Value\n",
    "\n",
    "Step 2: One-Hot Encode the Categorical Columns\n",
    "\n",
    "4. Combine Pipelines using ColumnTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "# Separate numerical and categorical columns\n",
    "numerical_cols = X_train.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_cols = X_train.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Numerical pipeline\n",
    "numerical_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Categorical pipeline\n",
    "categorical_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Combine pipelines\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_pipeline, numerical_cols),\n",
    "        ('cat', categorical_pipeline, categorical_cols)\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Build and Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append classifier to preprocessing pipeline\n",
    "# Now we have a full prediction pipeline\n",
    "pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                           ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))])\n",
    "\n",
    "# Train the model\n",
    "pipeline.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Make predictions\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Evaluate the accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Classification Report:\\n{report}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretation and Improvements\n",
    "Interpretation:\n",
    "\n",
    "The accuracy score and classification report give insights into how well the model performs on the test data.\n",
    "The classification report includes precision, recall, f1-score, and support, providing a comprehensive evaluation of the model’s performance for each class.\n",
    "Possible Improvements:\n",
    "\n",
    "Hyperparameter Tuning: Use techniques like GridSearchCV or RandomizedSearchCV to find the best hyperparameters for the Random Forest model.\n",
    "Feature Engineering: Investigate creating new features or transforming existing ones to improve model performance.\n",
    "Handling Imbalanced Data: If the dataset is imbalanced, consider using techniques like SMOTE or adjusting class weights in the Random Forest Classifier.\n",
    "Model Selection: Experiment with other models like Gradient Boosting, XGBoost, or neural networks to see if they perform better.\n",
    "Cross-Validation: Use cross-validation to ensure the model’s robustness and to prevent overfitting.\n",
    "This pipeline provides a strong foundation for automating feature engineering and handling missing values while building a robust machine learning model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2--\n",
    "Answer-- build a pipeline that includes both a Random Forest Classifier and a Logistic Regression Classifier, and then use a Voting Classifier to combine their predictions. We'll train the pipeline on the Iris dataset and evaluate its accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a pipeline for the Random Forest classifier\n",
    "rf_pipeline = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "])\n",
    "\n",
    "# Create a pipeline for the Logistic Regression classifier\n",
    "lr_pipeline = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', LogisticRegression(random_state=42))\n",
    "])\n",
    "\n",
    "# Create a Voting Classifier to combine the predictions from both classifiers\n",
    "voting_clf = VotingClassifier(estimators=[\n",
    "    ('rf', rf_pipeline),\n",
    "    ('lr', lr_pipeline)\n",
    "], voting='hard')\n",
    "\n",
    "# Train the combined pipeline\n",
    "voting_clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = voting_clf.predict(X_test)\n",
    "\n",
    "# Evaluate the accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred, target_names=iris.target_names)\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Classification Report:\\n{report}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation\n",
    "Loading the Iris dataset: The Iris dataset is a well-known dataset for classification tasks. It includes three classes of iris plants, with 50 samples each, where each class refers to a type of iris plant.\n",
    "\n",
    "Splitting the dataset: We split the dataset into training and test sets with an 80-20 split.\n",
    "\n",
    "Creating pipelines:\n",
    "\n",
    "Random Forest Pipeline: This includes standard scaling and the Random Forest classifier.\n",
    "Logistic Regression Pipeline: This includes standard scaling and the Logistic Regression classifier.\n",
    "Voting Classifier: The Voting Classifier combines the predictions from the Random Forest and Logistic Regression classifiers. We use 'hard' voting, which means the final prediction is based on the majority vote from the individual classifiers.\n",
    "\n",
    "Training the pipeline: We fit the Voting Classifier on the training data.\n",
    "\n",
    "Evaluating the model: We make predictions on the test set and evaluate the model's accuracy and classification report, which includes precision, recall, and F1-score for each class.\n",
    "\n",
    "Interpretation of Results\n",
    "Accuracy: The accuracy score gives a single metric for the overall performance of the model.\n",
    "Classification Report: This report provides detailed metrics for each class, helping us understand how well the model performs for each type of iris plant.\n",
    "Possible Improvements\n",
    "Hyperparameter Tuning: Optimize the hyperparameters of the Random Forest and Logistic Regression models using GridSearchCV or RandomizedSearchCV.\n",
    "Model Diversity: Add more diverse models to the Voting Classifier to potentially improve performance.\n",
    "Cross-Validation: Use cross-validation to ensure the model is not overfitting and performs well on unseen data.\n",
    "Feature Engineering: Investigate the impact of feature engineering and selection on model performance.\n",
    "This pipeline provides a robust framework for combining multiple classifiers to improve classification performance on the Iris dataset."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

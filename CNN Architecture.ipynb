{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Pooling and Padding in CNN\n",
    "\n",
    "# Pooling in CNN\n",
    "\"Pooling in CNN\"\n",
    "==\"Pooling is used in CNN to reduce the spatial dimensions of the input volume, which decreases the number of parameters and computations in the network, and controls overfitting.\"\n",
    "\n",
    "# Difference between min pooling and max pooling\n",
    "\"Max pooling takes the maximum value from the patch of the feature map covered by the filter, while average pooling takes the average value.\n",
    "\n",
    "# Padding in CNN\n",
    "\"Padding in CNN\"==\n",
    "Padding is used to control the spatial dimensions of the output volume. It helps preserve the spatial dimensions of the input after convolution, which is useful in deeper networks.\n",
    "\n",
    "# Zero-padding vs Valid-padding\n",
    "Zero-padding adds zeros around the input volume, maintaining the input size after convolution. Valid-padding (no padding) reduces the input size after convolution.\"\n",
    "\n",
    "# Exploring LeNet\n",
    "\n",
    "# LeNet-5 Overview\n",
    "\"LeNet-5 Overview==\n",
    "=LeNet-5, developed by Yann LeCun, is a pioneering convolutional neural network architecture designed for handwritten digit recognition. It consists of 7 layers including convolutional layers, subsampling layers, and fully connected layers.\n",
    "# Key components of LeNet-5\n",
    "\"Key components of LeNet-5\"\n",
    "print(\"1. Convolutional layers: Extract spatial features from the input image.\")\n",
    "\"2. Subsampling layers (average pooling): Reduce the spatial dimensions and the number of parameters.\"\n",
    "\"3. Fully connected layers: Perform classification based on extracted features.\"\n",
    "\"Advantages: Efficient for small images, early demonstration of CNN effectiveness.\n",
    "\"Limitations: Not suitable for large images, limited by hardware of its time.\"\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "\n",
    "# Load and preprocess the MNIST dataset\n",
    "(train_images, train_labels), (test_images, test_labels) = datasets.mnist.load_data()\n",
    "train_images, test_images = train_images / 255.0, test_images / 255.0\n",
    "\n",
    "# Reshape data to add a single channel\n",
    "train_images = train_images.reshape((train_images.shape[0], 28, 28, 1))\n",
    "test_images = test_images.reshape((test_images.shape[0], 28, 28, 1))\n",
    "\n",
    "# Implement LeNet-5\n",
    "lenet_model = models.Sequential([\n",
    "    layers.Conv2D(6, (5, 5), activation='tanh', input_shape=(28, 28, 1)),\n",
    "    layers.AveragePooling2D(),\n",
    "    layers.Conv2D(16, (5, 5), activation='tanh'),\n",
    "    layers.AveragePooling2D(),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(120, activation='tanh'),\n",
    "    layers.Dense(84, activation='tanh'),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile and train the model\n",
    "lenet_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "lenet_model.fit(train_images, train_labels, epochs=10, validation_data=(test_images, test_labels))\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = lenet_model.evaluate(test_images, test_labels)\n",
    "print(\"LeNet-5 Test Accuracy:\", test_acc)\n",
    "\n",
    "# Analyzing AlexNet\n",
    "\n",
    "# AlexNet Overview\n",
    "print(\"AlexNet Overview\")\n",
    "print(\"AlexNet, developed by Alex Krizhevsky, won the ImageNet Large Scale Visual Recognition Challenge in 2012. It has 8 layers: 5 convolutional layers followed by 3 fully connected layers.\")\n",
    "\n",
    "# Innovations in AlexNet\n",
    "print(\"Innovations in AlexNet\")\n",
    "print(\"1. Use of ReLU activation function for faster training.\")\n",
    "print(\"2. Overlapping max pooling for better generalization.\")\n",
    "print(\"3. Dropout layers to reduce overfitting.\")\n",
    "print(\"4. Data augmentation for improved model robustness.\")\n",
    "print(\"Convolutional layers extract features from input images, pooling layers reduce spatial dimensions, and fully connected layers perform classification.\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Define AlexNet architecture\n",
    "class AlexNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256 * 6 * 6, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# Set up data transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Initialize and train AlexNet\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AlexNet(num_classes=10).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(10):  # training for 10 epochs\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch [{epoch + 1}/10], Loss: {running_loss/len(train_loader)}\")\n",
    "\n",
    "# Evaluate AlexNet\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(\"AlexNet Test Accuracy:\", 100 * correct / total)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

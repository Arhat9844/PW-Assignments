{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a consolidated script that addresses all the questions using Python and Keras. This script covers the generation of images using DCGAN, fine-tuning a ResNet50 on CIFAR-10, and implementing a GAN from scratch to generate celebrity faces.\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.datasets import cifar10\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Flatten, Reshape, Conv2D, Conv2DTranspose, LeakyReLU, Dropout, BatchNormalization, Input, Softmax\n",
    "from keras.optimizers import Adam\n",
    "from keras.applications import ResNet50\n",
    "from keras.losses import BinaryCrossentropy\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import initializers\n",
    "from keras.layers import GlobalAveragePooling2D\n",
    "import keras\n",
    "\n",
    "# Question 1: DCGAN for Image Generation from Noise\n",
    "\n",
    "def build_generator():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(256*4*4, activation=\"relu\", input_dim=100))\n",
    "    model.add(Reshape((4, 4, 256)))\n",
    "    model.add(Conv2DTranspose(128, kernel_size=4, strides=2, padding=\"same\"))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Conv2DTranspose(64, kernel_size=4, strides=2, padding=\"same\"))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Conv2DTranspose(3, kernel_size=4, strides=2, padding=\"same\", activation='tanh'))\n",
    "    return model\n",
    "\n",
    "def build_discriminator():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(64, kernel_size=4, strides=2, input_shape=(32, 32, 3), padding=\"same\"))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv2D(128, kernel_size=4, strides=2, padding=\"same\"))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "def train_dcgan(generator, discriminator, combined, epochs=10000, batch_size=64, save_interval=1000):\n",
    "    (X_train, _), (_, _) = cifar10.load_data()\n",
    "    X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
    "\n",
    "    valid = np.ones((batch_size, 1))\n",
    "    fake = np.zeros((batch_size, 1))\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "        imgs = X_train[idx]\n",
    "\n",
    "        noise = np.random.normal(0, 1, (batch_size, 100))\n",
    "        gen_imgs = generator.predict(noise)\n",
    "\n",
    "        d_loss_real = discriminator.train_on_batch(imgs, valid)\n",
    "        d_loss_fake = discriminator.train_on_batch(gen_imgs, fake)\n",
    "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "        g_loss = combined.train_on_batch(noise, valid)\n",
    "\n",
    "        if epoch % save_interval == 0:\n",
    "            print(f\"{epoch} [D loss: {d_loss[0]} | D accuracy: {100*d_loss[1]}] [G loss: {g_loss}]\")\n",
    "            save_images(epoch, generator)\n",
    "\n",
    "def save_images(epoch, generator, examples=10, dim=(1, 10), figsize=(10, 1)):\n",
    "    noise = np.random.normal(0, 1, (examples, 100))\n",
    "    generated_images = generator.predict(noise)\n",
    "    generated_images = 0.5 * generated_images + 0.5\n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "    for i in range(examples):\n",
    "        plt.subplot(dim[0], dim[1], i+1)\n",
    "        plt.imshow(generated_images[i])\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"gan_generated_image_epoch_{epoch}.png\")\n",
    "    plt.close()\n",
    "\n",
    "generator = build_generator()\n",
    "discriminator = build_discriminator()\n",
    "discriminator.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5), metrics=['accuracy'])\n",
    "\n",
    "z = Input(shape=(100,))\n",
    "img = generator(z)\n",
    "discriminator.trainable = False\n",
    "valid = discriminator(img)\n",
    "\n",
    "combined = Model(z, valid)\n",
    "combined.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5))\n",
    "\n",
    "train_dcgan(generator, discriminator, combined)\n",
    "\n",
    "# Question 2: Fine-tuning ResNet50 on CIFAR-10\n",
    "\n",
    "def fine_tune_resnet50(num_classes, strategy):\n",
    "    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(32, 32, 3))\n",
    "    \n",
    "    if strategy == 'from_scratch':\n",
    "        for layer in base_model.layers:\n",
    "            layer.trainable = True\n",
    "    elif strategy == 'freeze_base':\n",
    "        for layer in base_model.layers:\n",
    "            layer.trainable = False\n",
    "    elif strategy == 'fine_tune_all':\n",
    "        for layer in base_model.layers:\n",
    "            layer.trainable = True\n",
    "    \n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    predictions = Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "    model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "y_train = keras.utils.to_categorical(y_train, 10)\n",
    "y_test = keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "datagen.fit(x_train)\n",
    "\n",
    "strategies = ['from_scratch', 'freeze_base', 'fine_tune_all']\n",
    "for strategy in strategies:\n",
    "    print(f\"Training with strategy: {strategy}\")\n",
    "    model = fine_tune_resnet50(10, strategy)\n",
    "    history = model.fit(datagen.flow(x_train, y_train, batch_size=64), epochs=50, validation_data=(x_test, y_test))\n",
    "    plt.plot(history.history['accuracy'], label=f'{strategy}_train_acc')\n",
    "    plt.plot(history.history['val_accuracy'], label=f'{strategy}_val_acc')\n",
    "\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Question 3: GAN for Celebrity Faces using Keras\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "def build_gan(generator, discriminator):\n",
    "    z = Input(shape=(100,))\n",
    "    img = generator(z)\n",
    "    discriminator.trainable = False\n",
    "    valid = discriminator(img)\n",
    "    combined = Model(z, valid)\n",
    "    combined.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5))\n",
    "    return combined\n",
    "\n",
    "def train_gan(generator, discriminator, combined, data_generator, epochs=10000, batch_size=64, save_interval=1000):\n",
    "    valid = np.ones((batch_size, 1))\n",
    "    fake = np.zeros((batch_size, 1))\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        imgs = data_generator.next()\n",
    "        imgs = (imgs - 127.5) / 127.5\n",
    "\n",
    "        noise = np.random.normal(0, 1, (batch_size, 100))\n",
    "        gen_imgs = generator.predict(noise)\n",
    "\n",
    "        d_loss_real = discriminator.train_on_batch(imgs, valid)\n",
    "        d_loss_fake = discriminator.train_on_batch(gen_imgs, fake)\n",
    "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "        g_loss = combined.train_on_batch(noise, valid)\n",
    "\n",
    "        if epoch % save_interval == 0:\n",
    "            print(f\"{epoch} [D loss: {d_loss[0]} | D accuracy: {100*d_loss[1]}] [G loss: {g_loss}]\")\n",
    "            save_images(epoch, generator)\n",
    "\n",
    "celeba_datagen = ImageDataGenerator(rescale=1./255).flow_from_directory(\n",
    "    'path/to/celeba/img_align_celeba',\n",
    "    target_size=(32, 32),\n",
    "    batch_size=64,\n",
    "    class_mode=None\n",
    ")\n",
    "\n",
    "gan_generator = build_generator()\n",
    "gan_discriminator = build_discriminator()\n",
    "gan_discriminator.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5), metrics=['accuracy'])\n",
    "gan_combined = build_gan(gan_generator, gan_discriminator)\n",
    "\n",
    "train_gan(gan_generator, gan_discriminator, gan_combined, celeba_datagen)\n",
    "\n",
    "# Note: This script assumes the path to the CelebA dataset is correctly specified.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CODING ANSWERS-\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Conv2DTranspose, Flatten, Reshape, LeakyReLU, BatchNormalization, Dropout\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. Data Augmentation Function for GAN Training\n",
    "def augment_data(dataset, rotation_range=20, flip_prob=0.5, crop_size=(32, 32)):\n",
    "    datagen = ImageDataGenerator(\n",
    "        rotation_range=rotation_range,\n",
    "        horizontal_flip=True,\n",
    "        width_shift_range=0.1,\n",
    "        height_shift_range=0.1,\n",
    "        zoom_range=0.2\n",
    "    )\n",
    "    augmented_data = datagen.flow(dataset, batch_size=len(dataset), shuffle=False)\n",
    "    return next(augmented_data)[0]\n",
    "\n",
    "# 2. Create a Simple Discriminator Model\n",
    "def build_discriminator(input_shape=(32, 32, 3)):\n",
    "    model = Sequential([\n",
    "        Conv2D(64, kernel_size=4, strides=2, input_shape=input_shape, padding=\"same\"),\n",
    "        LeakyReLU(alpha=0.2),\n",
    "        Dropout(0.25),\n",
    "        Conv2D(128, kernel_size=4, strides=2, padding=\"same\"),\n",
    "        LeakyReLU(alpha=0.2),\n",
    "        Dropout(0.25),\n",
    "        Flatten(),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# 3. Create a Generator Model\n",
    "def build_generator(latent_dim=100):\n",
    "    model = Sequential([\n",
    "        Dense(256*4*4, activation=\"relu\", input_dim=latent_dim),\n",
    "        Reshape((4, 4, 256)),\n",
    "        Conv2DTranspose(128, kernel_size=4, strides=2, padding=\"same\"),\n",
    "        BatchNormalization(momentum=0.8),\n",
    "        LeakyReLU(alpha=0.2),\n",
    "        Conv2DTranspose(64, kernel_size=4, strides=2, padding=\"same\"),\n",
    "        BatchNormalization(momentum=0.8),\n",
    "        LeakyReLU(alpha=0.2),\n",
    "        Conv2DTranspose(3, kernel_size=4, strides=2, padding=\"same\", activation='tanh')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# 4. Minimax Loss Function for GANs\n",
    "def minimax_loss(real_output, fake_output):\n",
    "    cross_entropy = BinaryCrossentropy(from_logits=True)\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss\n",
    "\n",
    "# 5. GAN Model Combining Generator and Discriminator\n",
    "def build_gan(generator, discriminator):\n",
    "    discriminator.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    discriminator.trainable = False\n",
    "\n",
    "    gan_input = tf.keras.Input(shape=(100,))\n",
    "    generated_image = generator(gan_input)\n",
    "    gan_output = discriminator(generated_image)\n",
    "    gan = Model(gan_input, gan_output)\n",
    "    gan.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "\n",
    "    return gan\n",
    "\n",
    "# Training Function for GAN\n",
    "def train_gan(generator, discriminator, gan, dataset, epochs=10000, batch_size=64, save_interval=1000):\n",
    "    (X_train, _), (_, _) = dataset\n",
    "    X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
    "\n",
    "    valid = np.ones((batch_size, 1))\n",
    "    fake = np.zeros((batch_size, 1))\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "        imgs = X_train[idx]\n",
    "\n",
    "        noise = np.random.normal(0, 1, (batch_size, 100))\n",
    "        gen_imgs = generator.predict(noise)\n",
    "\n",
    "        d_loss_real = discriminator.train_on_batch(imgs, valid)\n",
    "        d_loss_fake = discriminator.train_on_batch(gen_imgs, fake)\n",
    "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "        g_loss = gan.train_on_batch(noise, valid)\n",
    "\n",
    "        if epoch % save_interval == 0:\n",
    "            print(f\"{epoch} [D loss: {d_loss[0]} | D accuracy: {100*d_loss[1]}] [G loss: {g_loss}]\")\n",
    "            save_images(epoch, generator)\n",
    "\n",
    "def save_images(epoch, generator, examples=10, dim=(1, 10), figsize=(10, 1)):\n",
    "    noise = np.random.normal(0, 1, (examples, 100))\n",
    "    generated_images = generator.predict(noise)\n",
    "    generated_images = 0.5 * generated_images + 0.5\n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "    for i in range(examples):\n",
    "        plt.subplot(dim[0], dim[1], i+1)\n",
    "        plt.imshow(generated_images[i])\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"gan_generated_image_epoch_{epoch}.png\")\n",
    "    plt.close()\n",
    "\n",
    "# 6. Transfer Learning with GANs on the CIFAR-10 Dataset\n",
    "def transfer_learning_gan(dataset):\n",
    "    generator = build_generator()\n",
    "    discriminator = build_discriminator()\n",
    "    gan = build_gan(generator, discriminator)\n",
    "    train_gan(generator, discriminator, gan, dataset)\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "# Augment data for GAN training\n",
    "X_train_augmented = augment_data(X_train)\n",
    "\n",
    "# Build and train GAN\n",
    "generator = build_generator()\n",
    "discriminator = build_discriminator()\n",
    "gan = build_gan(generator, discriminator)\n",
    "train_gan(generator, discriminator, gan, (X_train_augmented, y_train))\n",
    "\n",
    "# Transfer learning GAN\n",
    "transfer_learning_gan((X_train_augmented, y_train))\n",
    "\n",
    "This code covers:\n",
    "\n",
    "Data augmentation for GAN training using random rotations, flipping, and cropping.\n",
    "A simple discriminator model for classifying images as real or fake.\n",
    "A generator model that uses transpose convolution to generate 32x32x3 images from random noise.\n",
    "A Minimax loss function for GANs.\n",
    "Combining the generator and discriminator into a GAN model and training it on the CIFAR-10 dataset.\n",
    "Implementing transfer learning for GANs on the CIFAR-10 dataset."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

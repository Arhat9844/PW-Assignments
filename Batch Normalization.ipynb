{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1--\n",
    "Answer-\n",
    "### Batch Normalization in ANN\n",
    "\n",
    "**Concept**:\n",
    "Batch normalization normalizes the input of each layer in a neural network to have a mean of zero and a variance of one. This stabilization of input distributions accelerates training and improves model performance.\n",
    "\n",
    "**Benefits**:\n",
    "1. Accelerates training.\n",
    "2. Reduces internal covariate shift.\n",
    "3. Mitigates vanishing/exploding gradients.\n",
    "4. Allows higher learning rates.\n",
    "5. Regularizes the model, potentially reducing the need for dropout.\n",
    "\n",
    "**Working Principle**:\n",
    "1. **Normalization**: For each mini-batch, normalize inputs by subtracting the batch mean and dividing by the batch standard deviation.\n",
    "2. **Learnable Parameters**: Introduce scale (gamma) and shift (beta) parameters to maintain the representation power. These parameters are learned during training, allowing the model to adapt the normalization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2--\n",
    "Answer-\n",
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Load and preprocess the MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "# Define and train a feedforward neural network without batch normalization\n",
    "model_without_bn = Sequential([\n",
    "    Flatten(input_shape=(28, 28)),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model_without_bn.compile(optimizer='adam',\n",
    "                         loss='sparse_categorical_crossentropy',\n",
    "                         metrics=['accuracy'])\n",
    "\n",
    "model_without_bn.fit(x_train, y_train, epochs=5, validation_data=(x_test, y_test))\n",
    "\n",
    "# Define and train a feedforward neural network with batch normalization\n",
    "model_with_bn = Sequential([\n",
    "    Flatten(input_shape=(28, 28)),\n",
    "    Dense(128),\n",
    "    BatchNormalization(),\n",
    "    tf.keras.layers.ReLU(),\n",
    "    Dense(64),\n",
    "    BatchNormalization(),\n",
    "    tf.keras.layers.ReLU(),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model_with_bn.compile(optimizer='adam',\n",
    "                      loss='sparse_categorical_crossentropy',\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "model_with_bn.fit(x_train, y_train, epochs=5, validation_data=(x_test, y_test))\n",
    "\n",
    "# Compare training and validation performance between models with and without batch normalization\n",
    "loss_without_bn, acc_without_bn = model_without_bn.evaluate(x_test, y_test)\n",
    "loss_with_bn, acc_with_bn = model_with_bn.evaluate(x_test, y_test)\n",
    "\n",
    "print(\"Model without Batch Normalization:\")\n",
    "print(f\"Loss: {loss_without_bn}, Accuracy: {acc_without_bn}\")\n",
    "\n",
    "print(\"Model with Batch Normalization:\")\n",
    "print(f\"Loss: {loss_with_bn}, Accuracy: {acc_with_bn}\")\n",
    "\n",
    "# Compare training and validation performance between models with and without batch normalization\n",
    "loss_without_bn, acc_without_bn = model_without_bn.evaluate(x_test, y_test)\n",
    "loss_with_bn, acc_with_bn = model_with_bn.evaluate(x_test, y_test)\n",
    "\n",
    "print(\"Model without Batch Normalization:\")\n",
    "print(f\"Loss: {loss_without_bn}, Accuracy: {acc_without_bn}\")\n",
    "\n",
    "print(\"Model with Batch Normalization:\")\n",
    "print(f\"Loss: {loss_with_bn}, Accuracy: {acc_with_bn}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discuss the impact of batch normalization on training process and performance\n",
    "\"\"\"\n",
    "Batch normalization significantly improves the training process and performance of the neural network. \n",
    "Without batch normalization, the model's performance may fluctuate due to internal covariate shift, leading to slower convergence and lower accuracy. \n",
    "Batch normalization stabilizes the training by normalizing inputs, accelerating convergence, and enabling the use of higher learning rates. \n",
    "This results in faster training and improved model generalization, as demonstrated by the higher accuracy achieved with batch normalization compared to the model without it.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3--\n",
    "Answer-\n",
    "### Experimentation and Analysis\n",
    "\n",
    "**Experimenting with Batch Sizes**:\n",
    "- Try different batch sizes (e.g., 32, 64, 128) during training.\n",
    "- Observe the effect on training dynamics (e.g., convergence speed, stability) and model performance (e.g., accuracy, loss).\n",
    "- Analyze how larger batch sizes affect training time and resource utilization.\n",
    "\n",
    "**Advantages of Batch Normalization**:\n",
    "1. **Stabilizes Training**: Reduces internal covariate shift, leading to more stable and faster convergence.\n",
    "2. **Enables Higher Learning Rates**: Normalizing inputs allows for the use of higher learning rates, accelerating training.\n",
    "3. **Regularization**: Acts as a form of regularization, potentially reducing the need for dropout and improving model generalization.\n",
    "4. **Improves Gradient Flow**: Mitigates vanishing/exploding gradients, facilitating better weight updates and learning dynamics.\n",
    "\n",
    "**Limitations of Batch Normalization**:\n",
    "1. **Batch Size Sensitivity**: Performance may degrade with very small batch sizes due to inaccurate estimates of batch statistics.\n",
    "2. **Increased Memory Consumption**: Requires additional memory to store batch statistics during training, impacting memory usage, especially for large networks or GPUs with limited memory.\n",
    "3. **Test-time Behavior**: During inference, batch normalization relies on batch statistics computed during training, which may lead to inconsistent results when dealing with single samples or small batches.\n",
    "4. **Not Always Necessary**: In some cases, simple architectures or small datasets may not benefit significantly from batch normalization, leading to overhead without noticeable performance improvement.\n",
    "\n",
    "Experimenting with different configurations and understanding these trade-offs can help optimize the use of batch normalization for specific tasks and architectures.\n",
    "CODE==\n",
    "# Experimenting with different batch sizes\n",
    "batch_sizes = [32, 64, 128]\n",
    "for batch_size in batch_sizes:\n",
    "    # Define and compile the model\n",
    "    model = Sequential([\n",
    "        Flatten(input_shape=(28, 28)),\n",
    "        Dense(128),\n",
    "        BatchNormalization(),\n",
    "        tf.keras.layers.ReLU(),\n",
    "        Dense(64),\n",
    "        BatchNormalization(),\n",
    "        tf.keras.layers.ReLU(),\n",
    "        Dense(10, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    # Train the model with the current batch size\n",
    "    model.fit(x_train, y_train, batch_size=batch_size, epochs=5, validation_data=(x_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantages of Batch Normalization\n",
    "\n",
    "1. Stabilizes Training: Reduces internal covariate shift, leading to more stable and faster convergence.\n",
    "2. Enables Higher Learning Rates: Normalizing inputs allows for the use of higher learning rates, accelerating training.\n",
    "3. Regularization: Acts as a form of regularization, potentially reducing the need for dropout and improving model generalization.\n",
    "4. Improves Gradient Flow: Mitigates vanishing/exploding gradients, facilitating better weight updates and learning dynamics.\n",
    "\n",
    "### Limitations of Batch Normalization\n",
    "\n",
    "1. Batch Size Sensitivity: Performance may degrade with very small batch sizes due to inaccurate estimates of batch statistics.\n",
    "2. Increased Memory Consumption: Requires additional memory to store batch statistics during training, impacting memory usage, especially for large networks or GPUs with limited memory.\n",
    "3. Test-time Behavior: During inference, batch normalization relies on batch statistics computed during training, which may lead to inconsistent results when dealing with single samples or small batches.\n",
    "4. Not Always Necessary: In some cases, simple architectures or small datasets may not benefit significantly from batch normalization, leading to overhead without noticeable performance improvement.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1--\n",
    "Answer--\n",
    "Lasso Regression (Least Absolute Shrinkage and Selection Operator) is a linear regression technique that performs both variable selection and regularization. By adding a penalty equivalent to the absolute value of the magnitude of coefficients, Lasso forces some coefficients to be exactly zero, thus selecting a simpler model. This contrasts with Ridge Regression, which uses an L2 penalty, and standard Linear Regression, which doesn't penalize coefficient size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2--\n",
    "Answer--\n",
    "The main advantage of using Lasso Regression in feature selection is its ability to automatically perform variable selection by shrinking some coefficients to exactly zero. This simplifies the model, enhances interpretability, and can improve prediction accuracy by removing irrelevant features, thus reducing overfitting and improving generalization to new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3--\n",
    "Answer--\n",
    "In Lasso Regression, coefficients are interpreted similarly to those in standard linear regression, indicating the relationship between predictors and the response variable. However, due to Lasso's L1 penalty, some coefficients may be exactly zero, indicating those features are excluded from the model. Non-zero coefficients represent selected features, with their magnitude reflecting the strength and direction of the relationship, accounting for the regularization effect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4--\n",
    "Answer--The primary tuning parameter in Lasso Regression is the regularization parameter, often denoted as alpha (α). Adjusting α controls the strength of the L1 penalty: a higher α increases regularization, leading to more coefficients shrinking to zero, enhancing feature selection but potentially underfitting. Conversely, a lower α reduces regularization, retaining more features but risking overfitting. Finding the optimal α balances model complexity and predictive performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q-5-\n",
    "Answer--Yes, Lasso Regression can be used for non-linear regression problems by incorporating polynomial features or using basis functions. \n",
    "\n",
    "1. **Feature Transformation**: Transform the original features into a higher-dimensional space using polynomial features, splines, or other basis functions.\n",
    "2. **Apply Lasso Regression**: Use the transformed features in a Lasso Regression model to perform regularization and feature selection on the new, higher-dimensional feature set.\n",
    "\n",
    "This captures non-linear relationships while maintaining regularization benefits.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q-6-\n",
    "Answer--Ridge Regression and Lasso Regression are both regularization techniques, but they differ in their penalty terms. Ridge Regression uses an L2 penalty, which adds the squared magnitude of coefficients to the loss function, shrinking coefficients but rarely setting them to zero. Lasso Regression uses an L1 penalty, adding the absolute value of coefficients, which can shrink some coefficients to zero, effectively performing variable selection and simplifying the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q7--\n",
    "Answer--Yes, Lasso Regression can handle multicollinearity in the input features by performing variable selection. The L1 penalty in Lasso Regression shrinks some coefficients to zero, effectively removing redundant or highly correlated features from the model. This helps in selecting a subset of relevant predictors, reducing multicollinearity and improving model interpretability and performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q8--\n",
    "Anwer----\n",
    "To choose the optimal value of the regularization parameter (lambda) in Lasso Regression, use cross-validation. The common approach is:\n",
    "\n",
    "1. **Grid Search**: Define a range of lambda values to evaluate.\n",
    "2. **Cross-Validation**: For each lambda, perform k-fold cross-validation, splitting the data into k subsets and training the model on k-1 while validating on the remaining subset.\n",
    "3. **Select Lambda**: Choose the lambda that minimizes the cross-validation error, balancing model complexity and performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

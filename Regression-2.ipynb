{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1--\n",
    "ANSWER--\n",
    "R-squared (𝑅^2) in linear regression measures the proportion of variance in the dependent variable explained by the independent variables. It's calculated as \n",
    "𝑅^2=1−𝑆𝑆𝑟𝑒𝑠/𝑆𝑆𝑡𝑜𝑡al \n",
    "An 𝑅^2 value of 1 indicates perfect fit, 0 means no explanatory power, and values between 0 and 1 indicate varying levels of explanatory power."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2--\n",
    "ANSWER--Adjusted R-squared adjusts the 𝑅^2 value for the number of predictors in a model, providing a more accurate measure, especially for multiple regression models. Unlike regularn 𝑅^2 which can artificially increase with more predictors, adjusted 𝑅^2 penalizes for adding non-significant predictors, preventing overestimation of model fit. It is calculated as:\n",
    "\n",
    "Adjusted \n",
    "𝑅^2=1−((1-𝑅^2)(𝑛−1)/𝑛−𝑘−1)\n",
    "\n",
    "where n is the number of observations and k is the number of predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3--\n",
    "ANSWER--\n",
    "Adjusted R-squared is more appropriate when comparing models with different numbers of predictors, as it accounts for the number of predictors and helps avoid overestimating the model's explanatory power by penalizing unnecessary complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4--\n",
    "ANSWER--\n",
    "RMSE (Root Mean Squared Error), MSE (Mean Squared Error), and MAE (Mean Absolute Error) are metrics for evaluating regression models:\n",
    "\n",
    "- **MSE**: Average of the squared differences between predicted and actual values.\n",
    "  MSE = (1/n) Σ (yi - ŷi)^2\n",
    "- **RMSE**: Square root of MSE, providing error in original units.\n",
    "  RMSE = √MSE\n",
    "- **MAE**: Average of absolute differences between predicted and actual values.\n",
    "  MAE = (1/n) Σ |yi - ŷi|\n",
    "\n",
    "These metrics represent model accuracy, with lower values indicating better fit.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5--\n",
    "ANSWER--\n",
    "Advantages and disadvantages of RMSE, MSE, and MAE in regression analysis:\n",
    "\n",
    "- **MSE**:\n",
    "  - Advantages: Penalizes larger errors, useful for optimization.\n",
    "  - Disadvantages: Sensitive to outliers, less interpretable due to squaring.\n",
    "  \n",
    "- **RMSE**:\n",
    "  - Advantages: Intuitive interpretation in original units, penalizes larger errors.\n",
    "  - Disadvantages: Sensitive to outliers, can obscure smaller errors.\n",
    "  \n",
    "- **MAE**:\n",
    "  - Advantages: Easy to interpret, less sensitive to outliers.\n",
    "  - Disadvantages: Does not penalize larger errors as effectively, less useful for optimization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q6--\n",
    "ANSWER--\n",
    "Lasso regularization (Least Absolute Shrinkage and Selection Operator) adds a penalty equal to the absolute value of the coefficients to the loss function, promoting sparsity by shrinking some coefficients to zero. This simplifies models by feature selection.\n",
    "\n",
    "Ridge regularization adds a penalty equal to the square of the coefficients, preventing overfitting by shrinking coefficients uniformly but not to zero.\n",
    "\n",
    "Lasso is more appropriate when feature selection is desired, while Ridge is better for handling multicollinearity and when all features are believed to be relevant.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q7--\n",
    "ANSWER--\n",
    "Regularized linear models prevent overfitting by adding a penalty term to the loss function, discouraging overly complex models with large coefficients. For example, Lasso and Ridge regression penalize large coefficients, encouraging simpler models. In Ridge regression, the penalty term is the sum of squared coefficients, while in Lasso, it's the sum of the absolute values of coefficients. These penalties shrink coefficients, reducing model complexity and preventing overfitting, thus improving generalization to unseen data. This regularization technique helps strike a balance between bias and variance, enhancing model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q8--\n",
    "ANSWER--\n",
    "Regularized linear models have limitations that may make them less suitable for certain regression analysis scenarios:\n",
    "\n",
    "Loss of Interpretability: Regularization techniques like Lasso may shrink coefficients to zero, leading to the exclusion of variables and loss of interpretability.\n",
    "\n",
    "Sensitivity to Hyperparameters: Performance can be sensitive to the choice of regularization parameter, requiring careful tuning.\n",
    "\n",
    "Assumption of Linearity: Regularized linear models assume linear relationships between predictors and the target variable, which may not always hold true.\n",
    "\n",
    "Difficulty with Non-linear Relationships: In cases of non-linear relationships, regularized linear models may not capture complex patterns effectively.\n",
    "\n",
    "Computational Complexity: The optimization process in regularized models can be computationally intensive, especially with large datasets.\n",
    "\n",
    "Not Suitable for Feature Engineering: Regularization discourages feature engineering efforts as it tends to reduce the impact of additional features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q9--\n",
    "ANSWER--Model B with an MAE of 8 would be chosen as the better performer. MAE measures the average absolute errors, making it less sensitive to outliers compared to RMSE, which squares errors. Thus, Model B's lower MAE indicates that, on average, its predictions deviate less from the actual values, making it more robust to extreme values. However, MAE may not penalize large errors as heavily as RMSE, potentially overlooking the impact of outliers in some cases.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q10--\n",
    "ANSWER--Choosing between Ridge and Lasso regularization depends on the specific characteristics of the data and the goals of the analysis. \n",
    "\n",
    "- Model A with Ridge regularization might be preferred when multicollinearity is a concern, as it tends to shrink coefficients uniformly without eliminating any completely.\n",
    "- Model B with Lasso regularization might be favored if feature selection is desired, as it tends to shrink some coefficients to zero, effectively performing feature selection.\n",
    "\n",
    "However, Lasso regularization can lead to sparsity, making it more interpretable but potentially discarding relevant features. Ridge regularization, on the other hand, might not eliminate irrelevant features entirely but can handle multicollinearity better. Thus, the choice should consider the balance between model interpretability, feature importance, and multicollinearity.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1--\n",
    "Answer--\n",
    "Linear SVM Formula:\n",
    "\n",
    "minimize 1/2 ||w||^2\n",
    "subject to: y_i(w • x_i + b) ≥ 1\n",
    "\n",
    "In this formula:\n",
    "- w represents the weight vector,\n",
    "- b is the bias term,\n",
    "- x_i are the input vectors,\n",
    "- y_i are the corresponding class labels.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2--\n",
    "Answer--\n",
    "\n",
    "The objective function of a linear Support Vector Machine (SVM) is to find the hyperplane that best separates the classes in a linearly separable dataset by maximizing the margin between the classes. This can be formulated as an optimization problem:\n",
    "minimize (1/2) ||w||^2\n",
    "subject to: y_i(w • x_i + b) ≥ 1 for all i\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3--\n",
    "Answer--\n",
    "### Kernel Trick in SVM\n",
    "\n",
    "The kernel trick in Support Vector Machines (SVM) allows the algorithm to operate in high-dimensional spaces without explicitly computing the coordinates of the data in those spaces. Instead, it uses kernel functions to calculate the dot products between the images of all pairs of data points in the feature space.\n",
    "\n",
    "Mathematically, for a given kernel function K(x_i, x_j), the SVM optimization problem can be transformed using this kernel function instead of the dot product of the original feature vectors:\n",
    "\n",
    "K(x_i, x_j) = φ(x_i) • φ(x_j)\n",
    "\n",
    "where:\n",
    "- K(x_i, x_j) is the kernel function,\n",
    "- φ(x) is the feature mapping function.\n",
    "\n",
    "Common kernel functions include:\n",
    "- **Linear kernel**: K(x_i, x_j) = x_i • x_j\n",
    "- **Polynomial kernel**: K(x_i, x_j) = (x_i • x_j + c)^d\n",
    "- **Radial basis function (RBF) or Gaussian kernel**: K(x_i, x_j) = exp(-γ ||x_i - x_j||^2)\n",
    "- **Sigmoid kernel**: K(x_i, x_j) = tanh(α x_i • x_j + c)\n",
    "\n",
    "The kernel trick enables the SVM to find a separating hyperplane in a higher-dimensional space without the computational burden of working directly in that space.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4--\n",
    "Answer--Role of Support Vectors in SVM\n",
    "Support vectors are the critical elements of the dataset in a Support Vector Machine (SVM). They are the data points that lie closest to the decision boundary (or hyperplane) and directly influence its position and orientation. The SVM algorithm aims to find the optimal hyperplane that maximizes the margin, which is the distance between the hyperplane and the nearest data points from each class. These nearest data points are the support vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5--\n",
    "Answer--# SVM Concepts: Hyperplane, Marginal Plane, Soft Margin, and Hard Margin\n",
    "\n",
    "## Generating Example Data\n",
    "\n",
    "Let's use Python with `matplotlib` and `scikit-learn` to illustrate these concepts. We'll generate some sample data and create the respective SVM models.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# Generate sample data\n",
    "X, y = make_blobs(n_samples=100, centers=2, random_state=6)\n",
    "\n",
    "# Create a hard margin SVM (large C value)\n",
    "clf_hard = svm.SVC(kernel='linear', C=1e5)\n",
    "clf_hard.fit(X, y)\n",
    "\n",
    "# Create a soft margin SVM (smaller C value)\n",
    "clf_soft = svm.SVC(kernel='linear', C=1.0)\n",
    "clf_soft.fit(X, y)\n",
    "\n",
    "# Function to plot decision boundary and margins\n",
    "def plot_svm(clf, X, y, title):\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, s=30, cmap=plt.cm.Paired)\n",
    "    # Plot support vectors\n",
    "    plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], s=100, facecolors='none', edgecolors='k')\n",
    "\n",
    "    ax = plt.gca()\n",
    "    xlim = ax.get_xlim()\n",
    "    ylim = ax.get_ylim()\n",
    "\n",
    "    xx = np.linspace(xlim[0], xlim[1], 30)\n",
    "    yy = np.linspace(ylim[0], ylim[1], 30)\n",
    "    YY, XX = np.meshgrid(yy, xx)\n",
    "    xy = np.vstack([XX.ravel(), YY.ravel()]).T\n",
    "    Z = clf.decision_function(xy).reshape(XX.shape)\n",
    "\n",
    "    ax.contour(XX, YY, Z, colors='k', levels=[-1, 0, 1], alpha=0.5,\n",
    "               linestyles=['--', '-', '--'])\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "# Plotting the hard margin SVM\n",
    "plot_svm(clf_hard, X, y, \"Hard Margin SVM\")\n",
    "\n",
    "# Plotting the soft margin SVM\n",
    "plot_svm(clf_soft, X, y, \"Soft Margin SVM\")\n",
    "\n",
    "Hyperplane: The decision boundary separating the classes.\n",
    "Marginal Planes: Parallel planes to the hyperplane touching the nearest points from each class.\n",
    "Hard Margin: Requires perfect separation of classes with no points inside the margin.\n",
    "Soft Margin: Allows some points to be inside the margin or misclassified for better generalization on non-linearly separable data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q6--\n",
    "Answer--# SVM Implementation through Iris Dataset\n",
    "\n",
    "## Steps:\n",
    "1. Load the iris dataset from the scikit-learn library and split it into a training set and a testing set.\n",
    "2. Train a linear SVM classifier on the training set and predict the labels for the testing set.\n",
    "3. Compute the accuracy of the model on the testing set.\n",
    "4. Plot the decision boundaries of the trained model using two of the features.\n",
    "5. Try different values of the regularization parameter C and see how it affects the performance of the model.\n",
    "\n",
    "### 1. Load the Iris Dataset and Split it into Training and Testing Sets\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, :2]  # We will use only the first two features for visualization\n",
    "y = iris.target\n",
    "\n",
    "# Split the dataset into a training set and a testing set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "2. Train a Linear SVM Classifier\n",
    "# Train a linear SVM classifier\n",
    "clf = SVC(kernel='linear', C=1.0)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for the testing set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "3. Compute the Accuracy of the Model\n",
    "# Compute the accuracy of the model on the testing set\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "4. Plot the Decision Boundaries of the Trained Model\n",
    "# Function to plot decision boundaries\n",
    "def plot_decision_boundaries(X, y, model, title):\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),\n",
    "                         np.arange(y_min, y_max, 0.02))\n",
    "    \n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    plt.contourf(xx, yy, Z, alpha=0.3)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k', marker='o')\n",
    "    plt.title(title)\n",
    "    plt.xlabel(iris.feature_names[0])\n",
    "    plt.ylabel(iris.feature_names[1])\n",
    "    plt.show()\n",
    "\n",
    "# Plot the decision boundaries\n",
    "plot_decision_boundaries(X_test, y_test, clf, 'SVM Decision Boundaries (C=1.0)')\n",
    "\n",
    "5. Try Different Values of the Regularization Parameter C\n",
    "# Train and evaluate the model with different values of C\n",
    "C_values = [0.01, 0.1, 1, 10, 100]\n",
    "for C in C_values:\n",
    "    clf = SVC(kernel='linear', C=C)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f'C={C}: Accuracy: {accuracy * 100:.2f}%')\n",
    "    plot_decision_boundaries(X_test, y_test, clf, f'SVM Decision Boundaries (C={C})')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bonus task--\n",
    "# Implementing a Linear SVM Classifier from Scratch\n",
    "\n",
    "## Steps:\n",
    "1. Load the Iris dataset and preprocess it.\n",
    "2. Implement the linear SVM classifier from scratch.\n",
    "3. Train the custom SVM classifier on the training set.\n",
    "4. Evaluate its performance on the testing set.\n",
    "5. Compare the performance with the scikit-learn implementation.\n",
    "\n",
    "### 1. Load the Iris Dataset and Preprocess it\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Split the dataset into a training set and a testing set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "2. Implement the Linear SVM Classifier from Scratch\n",
    "\n",
    "class LinearSVM:\n",
    "    def __init__(self, learning_rate=0.001, lambda_param=0.01, num_iterations=1000):\n",
    "        self.lr = learning_rate\n",
    "        self.lambda_param = lambda_param\n",
    "        self.num_iter = num_iterations\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        y_ = np.where(y <= 0, -1, 1)\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "\n",
    "        for _ in range(self.num_iter):\n",
    "            for idx, x_i in enumerate(X):\n",
    "                condition = y_[idx] * (np.dot(x_i, self.weights) - self.bias) >= 1\n",
    "                if condition:\n",
    "                    self.weights -= self.lr * (2 * self.lambda_param * self.weights)\n",
    "                else:\n",
    "                    self.weights -= self.lr * (2 * self.lambda_param * self.weights - np.dot(x_i, y_[idx]))\n",
    "                    self.bias -= self.lr * y_[idx]\n",
    "\n",
    "    def predict(self, X):\n",
    "        approx = np.dot(X, self.weights) - self.bias\n",
    "        return np.sign(approx)\n",
    "3. Train the Custom SVM Classifier on the Training Set\n",
    "# Instantiate and train the custom SVM classifier\n",
    "custom_svm = LinearSVM()\n",
    "custom_svm.fit(X_train, y_train)\n",
    "\n",
    "4. Evaluate its Performance on the Testing Set\n",
    "# Predict using the custom SVM classifier\n",
    "y_pred_custom = custom_svm.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy_custom = np.mean(y_pred_custom == y_test)\n",
    "print(f'Custom SVM Accuracy: {accuracy_custom * 100:.2f}%')\n",
    "\n",
    "5. Compare Performance with the scikit-learn Implementation\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Instantiate and train the scikit-learn SVM classifier\n",
    "sklearn_svm = SVC(kernel='linear')\n",
    "sklearn_svm.fit(X_train, y_train)\n",
    "\n",
    "# Predict using the scikit-learn SVM classifier\n",
    "y_pred_sklearn = sklearn_svm.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy_sklearn = np.mean(y_pred_sklearn == y_test)\n",
    "print(f'Scikit-learn SVM Accuracy: {accuracy_sklearn * 100:.2f}%')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "2021.11.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1--\n",
    "Answer--\n",
    "Linear Regression:\n",
    "\n",
    "Linear regression is used when the dependent variable is continuous and has a linear relationship with the independent variable(s).\n",
    "It predicts the value of the dependent variable based on the values of the independent variables.\n",
    "The output of linear regression is a continuous value, typically representing a quantity or a magnitude.\n",
    "Example: Predicting house prices based on features like size, number of bedrooms, location, etc.\n",
    "Logistic Regression:\n",
    "\n",
    "Logistic regression is used when the dependent variable is categorical and binary (two classes) or ordinal (more than two classes), and it models the probability that an instance belongs to a particular category.\n",
    "It estimates the probability of a binary outcome based on one or more predictor variables.\n",
    "The output of logistic regression is a probability score between 0 and 1, which can be interpreted as the likelihood of the event occurring.\n",
    "Example: Predicting whether an email is spam or not spam based on features like sender, subject line, and content. Here, the outcome is binary: either the email is spam (1) or not spam (0).\n",
    "In summary, linear regression is suitable for predicting continuous outcomes, while logistic regression is used for binary or ordinal categorical outcomes where the goal is to model the probability of occurrence of an event. Logistic regression is more appropriate when dealing with classification problems where the dependent variable is categorical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2--\n",
    "Answer--\n",
    "In logistic regression, the cost function, also known as the logistic loss or cross-entropy loss, measures the difference between the predicted probabilities and the actual binary outcomes.\n",
    "The goal of optimization in logistic regression is to minimize this cost function to find the optimal parameters \n",
    "The goal of optimization in logistic regression is to minimize this cost function to find the optimal parameters that best fit the data. This is typically done using optimization algorithms such as gradient descent or its variants."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3--\n",
    "Answer--Regularization in logistic regression adds a penalty term to the cost function to prevent overfitting. L1 regularization (Lasso) penalizes the absolute values of coefficients, promoting sparsity and feature selection. L2 regularization (Ridge) penalizes the squared magnitudes of coefficients, encouraging smaller but non-zero coefficients. By penalizing large coefficients, regularization discourages overly complex models that fit noise in the data, leading to improved generalization performance on unseen data. The regularization parameter controls the trade-off between model complexity and fitting the training data. Overall, regularization helps logistic regression models generalize better by promoting simpler and more robust models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4--\n",
    "Answer--\n",
    "The ROC (Receiver Operating Characteristic) curve illustrates the trade-off between sensitivity and specificity for a binary classifier. It plots the true positive rate against the false positive rate at various thresholds. In logistic regression, it evaluates model performance by measuring how well it discriminates between classes across different threshold values.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5--\n",
    "Answer--\n",
    "Some common techniques for feature selection in logistic regression are:\n",
    "\n",
    "1.Forward selection\n",
    "2.Backward elimination\n",
    "3.Recursive feature elimination\n",
    "4.Lasso regularization\n",
    "5.Principal Component Analysis (PCA)\n",
    "These techniques help improve model performance by reducing overfitting, minimizing the risk of multicollinearity, and enhancing interpretability by selecting the most relevant features. They streamline the model, focusing on the most informative predictors, leading to better generalization and predictive accuracy.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q6--\n",
    "Answer--\n",
    "Imbalanced datasets in logistic regression can be handled through various strategies:\n",
    "\n",
    "Resampling techniques like oversampling minority class instances or undersampling majority class instances.\n",
    "Synthetic data generation methods such as SMOTE (Synthetic Minority Over-sampling Technique).\n",
    "Algorithmic adjustments like setting class weights to penalize misclassifications of the minority class.\n",
    "Ensemble methods like bagging or boosting with resampling strategies.\n",
    "These approaches help the model learn from imbalanced data more effectively, improving classification performance.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q7--\n",
    "Answer--\n",
    "Common issues in logistic regression include multicollinearity, overfitting, and handling non-linear relationships. Multicollinearity, where independent variables are highly correlated, can be addressed by removing correlated variables, using regularization techniques like Ridge regression, or performing dimensionality reduction. Regularization penalizes large coefficients, mitigating multicollinearity's impact. Additionally, techniques like PCA can transform variables into uncorrelated components, reducing multicollinearity's effects while preserving most of the original information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
